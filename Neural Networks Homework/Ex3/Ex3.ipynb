{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Part a to e"
      ],
      "metadata": {
        "id": "sbmAVR1md2TP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQAtj3oOD26t"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "####### Part a to e\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1.0/(1.0 + np.exp(-x))\n",
        "\n",
        "def sigmoid_prime(x):\n",
        "    return (1.0/(1.0 + np.exp(-x))) * (1 - (1.0/(1.0 + np.exp(-x))))\n",
        "\n",
        "def tanh(x):\n",
        "    return np.tanh(x)\n",
        "\n",
        "def tanh_prime(x):\n",
        "    return 1 - np.power(tanh(x),2)\n",
        "\n",
        "\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers, activation='tanh'):\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = sigmoid\n",
        "            self.activation_prime = sigmoid_prime\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = tanh\n",
        "            self.activation_prime = tanh_prime\n",
        "\n",
        "        # Set weights\n",
        "        self.weights = []\n",
        "        # layers = [2,2,1]\n",
        "        # range of weight values (-1,1)\n",
        "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
        "        for i in range(1, len(layers) - 1):\n",
        "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
        "            self.weights.append(r)\n",
        "        # output layer - random((2+1, 1)) : 3 x 1\n",
        "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
        "        self.weights.append(r)\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.02, epochs=1000000):\n",
        "        # Add column of ones to X\n",
        "        # This is to add the bias unit to the input layer\n",
        "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
        "        X = np.concatenate((ones.T, X), axis=1)\n",
        "        error_arr = []\n",
        "        temp = 0\n",
        "        for k in range(epochs):\n",
        "            if k % 10000 == 0: print('epochs:', k)\n",
        "            \n",
        "            i = np.random.randint(X.shape[0])\n",
        "            a = [X[i]]\n",
        "\n",
        "            for l in range(len(self.weights)):\n",
        "                dot_value = np.dot(a[l], self.weights[l])\n",
        "                activation = self.activation(dot_value)\n",
        "                a.append(activation)\n",
        "            # output layer\n",
        "            error = y[i] - a[-1]\n",
        "            if k % 500 == 0: \n",
        "              print('error = ', error)\n",
        "              error_arr.append(error)\n",
        "              temp = temp + 1\n",
        "          \n",
        "\n",
        "            deltas = [error * self.activation_prime(a[-1])]\n",
        "\n",
        "            # we need to begin at the second to last layer \n",
        "            # (a layer before the output layer)\n",
        "            for l in range(len(a) - 2, 0, -1): \n",
        "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
        "\n",
        "            # reverse\n",
        "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "            deltas.reverse()\n",
        "\n",
        "            # backpropagation\n",
        "            # 1. Multiply its output delta and input activation \n",
        "            #    to get the gradient of the weight.\n",
        "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
        "            for i in range(len(self.weights)):\n",
        "                layer = np.atleast_2d(a[i])\n",
        "                delta = np.atleast_2d(deltas[i])\n",
        "                self.weights[i] += learning_rate * layer.T*delta\n",
        "\n",
        "        epoch_plot = 500.*(np.arange(temp) + 1)\n",
        "        plt.plot(epoch_plot,error_arr)\n",
        "\n",
        "            \n",
        "    def predict(self, x): \n",
        "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
        "        for l in range(0, len(self.weights)):\n",
        "            a = self.activation(np.dot(a, self.weights[l]))\n",
        "        return a\n",
        "\n",
        "##### code for \"XOR\" with tanh activation function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,2,1],'tanh')\n",
        "\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part f section 1 (Train the model for \"OR\" with tanh activation function)\n",
        "\n"
      ],
      "metadata": {
        "id": "p4q4kw-SeDCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### code for \"OR\" with tanh activation function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,2,1],'tanh')\n",
        "\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "    y = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ],
      "metadata": {
        "id": "2Xt4Ea5weUUG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part f section 2 (Train the model for \"AND\" with tanh activation function)\n"
      ],
      "metadata": {
        "id": "sM7Mg8L1fD-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### code for \"AND\" with activation tanh activation function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,2,1],'tanh')\n",
        "\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "    y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ],
      "metadata": {
        "id": "yaXWb5hUfL5c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part g section 1 (Train the model for \"XOR\" with logistic activation function)"
      ],
      "metadata": {
        "id": "NEzbcFcsgC1-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### code for \"XOR\" with activation logistic activation function\n",
        "class NeuralNetwork:\n",
        "\n",
        "    def __init__(self, layers, activation='tanh'):\n",
        "        if activation == 'sigmoid':\n",
        "            self.activation = sigmoid\n",
        "            self.activation_prime = sigmoid_prime\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = tanh\n",
        "            self.activation_prime = tanh_prime\n",
        "\n",
        "        # Set weights\n",
        "        self.weights = []\n",
        "        # layers = [2,2,1]\n",
        "        # range of weight values (-1,1)\n",
        "        # input and hidden layers - random((2+1, 2+1)) : 3 x 3\n",
        "        for i in range(1, len(layers) - 1):\n",
        "            r = 2*np.random.random((layers[i-1] + 1, layers[i] + 1)) -1\n",
        "            self.weights.append(r)\n",
        "        # output layer - random((2+1, 1)) : 3 x 1\n",
        "        r = 2*np.random.random( (layers[i] + 1, layers[i+1])) - 1\n",
        "        self.weights.append(r)\n",
        "\n",
        "    def fit(self, X, y, learning_rate=0.02, epochs=1000000):\n",
        "        # Add column of ones to X\n",
        "        # This is to add the bias unit to the input layer\n",
        "        ones = np.atleast_2d(np.ones(X.shape[0]))\n",
        "        X = np.concatenate((ones.T, X), axis=1)\n",
        "        error_arr = []\n",
        "        temp = 0\n",
        "        for k in range(epochs):\n",
        "            if k % 100000 == 0: print('epochs:', k)\n",
        "            \n",
        "            i = np.random.randint(X.shape[0])\n",
        "            a = [X[i]]\n",
        "\n",
        "            for l in range(len(self.weights)):\n",
        "                dot_value = np.dot(a[l], self.weights[l])\n",
        "                activation = self.activation(dot_value)\n",
        "                a.append(activation)\n",
        "            # output layer\n",
        "            error = y[i] - a[-1]\n",
        "            if k % 5000 == 0: \n",
        "              print('error = ', error)\n",
        "              error_arr.append(error)\n",
        "              temp = temp + 1\n",
        "          \n",
        "\n",
        "            deltas = [error * self.activation_prime(a[-1])]\n",
        "\n",
        "            # we need to begin at the second to last layer \n",
        "            # (a layer before the output layer)\n",
        "            for l in range(len(a) - 2, 0, -1): \n",
        "                deltas.append(deltas[-1].dot(self.weights[l].T)*self.activation_prime(a[l]))\n",
        "\n",
        "            # reverse\n",
        "            # [level3(output)->level2(hidden)]  => [level2(hidden)->level3(output)]\n",
        "            deltas.reverse()\n",
        "\n",
        "            # backpropagation\n",
        "            # 1. Multiply its output delta and input activation \n",
        "            #    to get the gradient of the weight.\n",
        "            # 2. Subtract a ratio (percentage) of the gradient from the weight.\n",
        "            for i in range(len(self.weights)):\n",
        "                layer = np.atleast_2d(a[i])\n",
        "                delta = np.atleast_2d(deltas[i])\n",
        "                self.weights[i] += learning_rate * layer.T*delta\n",
        "\n",
        "        epoch_plot = 500.*(np.arange(temp) + 1)\n",
        "        plt.plot(epoch_plot,error_arr)\n",
        "         \n",
        "\n",
        "    def predict(self, x): \n",
        "        a = np.concatenate((np.ones(1).T, np.array(x)))      \n",
        "        for l in range(0, len(self.weights)):\n",
        "            a = self.activation(np.dot(a, self.weights[l]))\n",
        "        return a\n",
        "        \n",
        "##### code for \"AND\" with activation logistic activation function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,200,1],'sigmoid')\n",
        "\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "    y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ],
      "metadata": {
        "id": "Y6LZJbviiodd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part g section 1 (Train the model for \"OR\" with logistic activation function)\n"
      ],
      "metadata": {
        "id": "d79Q09epFOTb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### code for \"OR\" with activation logistic activation function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,200,1],'sigmoid')\n",
        "\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "    y = np.array([[0], [1], [1], [1]])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ],
      "metadata": {
        "id": "uLLJWj5_FiL4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part g section 3 (Train the model for \"AND\" with logistic activation function)\n"
      ],
      "metadata": {
        "id": "7uR1V2pHvO0R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "##### code for \"AND\" with activation logistic activation function\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    nn = NeuralNetwork([2,200,1],'sigmoid')\n",
        "\n",
        "    X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "\n",
        "    y = np.array([[0], [0], [0], [1]])\n",
        "\n",
        "    nn.fit(X, y)\n",
        "\n",
        "    for e in X:\n",
        "        print(e,nn.predict(e))"
      ],
      "metadata": {
        "id": "ZQHSEh-YvU0G"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.2"
    },
    "colab": {
      "name": "Ex3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}